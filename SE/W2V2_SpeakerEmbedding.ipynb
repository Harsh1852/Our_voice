{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e200a02-117f-443a-baf9-b1b217cd7436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229.40665745735168\n"
     ]
    }
   ],
   "source": [
    "from dataloader import preprocess_example\n",
    "import time\n",
    "st = time.time()\n",
    "df = preprocess_example()\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bb64b9-eab5-4be2-a36f-295e2eacad54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['225' '226' '227' '228' '229' '230' '231' '232' '233' '234' '236' '237'\n",
      " '238' '239' '240' '241' '243' '244' '245' '246' '247' '248' '249' '250'\n",
      " '251' '252' '253' '254' '255' '256' '257' '258' '259' '260' '261' '262'\n",
      " '263' '264' '265' '266' '267' '268' '269' '270' '271' '272' '273' '274'\n",
      " '275' '276' '277' '278' '279' '280' '281' '282' '283' '284' '285' '286'\n",
      " '287' '288' '292' '293' '294' '295' '297' '298' '299' '300' '301' '302'\n",
      " '303' '304' '305' '306']\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75]\n",
      "Processing all waveforms to length 260948\n",
      "Original length range: 58544 to 794730\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(np.unique(df['speaker_id']))\n",
    "\n",
    "# Create a mapping of unique values to continuous numbers\n",
    "unique_speaker_ids = df['speaker_id'].unique()\n",
    "speaker_id_mapping = {speaker_id: idx for idx, speaker_id in enumerate(unique_speaker_ids)}\n",
    "\n",
    "# Replace the column values using the mapping\n",
    "df['speaker_id'] = df['speaker_id'].map(speaker_id_mapping)\n",
    "\n",
    "print(np.unique(df['speaker_id']))\n",
    "\n",
    "# Add this code right before you create the datasets and DataLoaders\n",
    "\n",
    "# Find a good fixed length (median or max with a safety margin)\n",
    "waveform_lengths = [len(waveform) for waveform in df['speech']]\n",
    "median_length = int(np.median(waveform_lengths))\n",
    "p95_length = int(np.percentile(waveform_lengths, 95))  # 95th percentile\n",
    "target_length = p95_length  # Or choose median_length if you prefer\n",
    "\n",
    "print(f\"Processing all waveforms to length {target_length}\")\n",
    "print(f\"Original length range: {min(waveform_lengths)} to {max(waveform_lengths)}\")\n",
    "\n",
    "# Process all waveforms\n",
    "for i in range(len(df)):\n",
    "    waveform = df.iloc[i]['speech']\n",
    "    \n",
    "    # Convert to tensor if needed\n",
    "    if not isinstance(waveform, torch.Tensor):\n",
    "        waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "    \n",
    "    current_length = waveform.shape[0]\n",
    "    \n",
    "    if current_length > target_length:\n",
    "        # Either truncate from beginning, center, or with random offset\n",
    "        # Center truncation:\n",
    "        start = (current_length - target_length) // 2\n",
    "        waveform = waveform[start:start + target_length]\n",
    "    elif current_length < target_length:\n",
    "        # Pad with zeros\n",
    "        padding_size = target_length - current_length\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding_size))\n",
    "    \n",
    "    # Update the dataframe\n",
    "    df.at[i, 'speech'] = waveform\n",
    "\n",
    "# Now your df['speech'] contains fixed-length waveforms\n",
    "# Continue with your original code without changing the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294358f9-2d7a-416e-b165-2ff4ae6f4210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75]\n",
      "13    502\n",
      "10    492\n",
      "37    483\n",
      "32    481\n",
      "23    481\n",
      "     ... \n",
      "11    339\n",
      "22    335\n",
      "29    319\n",
      "71    311\n",
      "0     231\n",
      "Name: speaker_id, Length: 76, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(df['speaker_id']))\n",
    "# Count the occurrences of unique values\n",
    "occurrences = df['speaker_id'].value_counts()\n",
    "print(occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f014412b-2825-469c-991b-ed09f732f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Custom dataset for speaker data\n",
    "class SpeakerDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        waveform = self.df.iloc[idx]['speech']  # Already preprocessed waveform\n",
    "        speaker_id = self.df.iloc[idx]['speaker_id']\n",
    "        instance_id = self.df.iloc[idx]['instance_id']\n",
    "        \n",
    "        # Convert speaker_id to numeric if it's not already\n",
    "        if not isinstance(speaker_id, (int, np.integer)):\n",
    "            speaker_id = int(speaker_id)\n",
    "        \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "            \n",
    "        return waveform, speaker_id\n",
    "\n",
    "# Data augmentation function\n",
    "def apply_augmentation(waveform):\n",
    "    \"\"\"Apply random augmentations to a speech waveform\"\"\"\n",
    "    # Ensure waveform is on CPU for torchaudio operations\n",
    "    device = waveform.device\n",
    "    waveform = waveform.cpu()\n",
    "    \n",
    "    # Random time shift\n",
    "    if random.random() > 0.5:\n",
    "        shift = int(random.random() * (waveform.shape[-1] // 10))\n",
    "        waveform = torch.roll(waveform, shift, dims=-1)\n",
    "    \n",
    "    # Random background noise (Gaussian)\n",
    "    if random.random() > 0.7:\n",
    "        noise_level = 0.005 * random.random()\n",
    "        noise = torch.randn_like(waveform) * noise_level\n",
    "        waveform = waveform + noise\n",
    "    \n",
    "    # Random volume change\n",
    "    if random.random() > 0.7:\n",
    "        volume_factor = 0.8 + 0.4 * random.random()  # 0.8 to 1.2\n",
    "        waveform = waveform * volume_factor\n",
    "        \n",
    "    # Ensure values are in valid range\n",
    "    waveform = torch.clamp(waveform, -1.0, 1.0)\n",
    "    \n",
    "    # Return to original device\n",
    "    return waveform.to(device)\n",
    "\n",
    "# Speaker Embedding Model using Transfer Learning\n",
    "class SpeakerEmbeddingModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, num_speakers=None, unfreeze_layers=2):\n",
    "        super().__init__()\n",
    "        # Load pretrained wav2vec2 model\n",
    "        bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "        self.wav2vec = bundle.get_model()\n",
    "        \n",
    "        # Freeze most of the pretrained model\n",
    "        for param in self.wav2vec.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze the final few transformer layers for fine-tuning\n",
    "        for i in range(unfreeze_layers):\n",
    "            for param in self.wav2vec.encoder.transformer.layers[-1-i].parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Get the output dimension of wav2vec2\n",
    "        self.wav2vec_dim = self.wav2vec.encoder.transformer.layers[0].final_layer_norm.normalized_shape[0]\n",
    "        \n",
    "        # Attention-based pooling\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.wav2vec_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Project to embedding dimension\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.wav2vec_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Optional classifier for speaker identification\n",
    "        self.num_speakers = num_speakers\n",
    "        if num_speakers:\n",
    "            self.classifier = nn.Linear(embedding_dim, num_speakers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        x: tensor of shape [batch_size, time]\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            # wav2vec2 requires 16kHz waveforms\n",
    "            features, _ = self.wav2vec.extract_features(x)\n",
    "        \n",
    "        # The last layer features\n",
    "        x = features[-1]  # [batch_size, time, feature_dim]\n",
    "        \n",
    "        # Apply attention pooling\n",
    "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
    "        x = torch.sum(x * attention_weights, dim=1)  # [batch_size, feature_dim]\n",
    "        \n",
    "        # Project to embedding space\n",
    "        embedding = self.projector(x)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # L2 normalization\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        # Classification if needed\n",
    "        if self.num_speakers:\n",
    "            logits = self.classifier(embedding)\n",
    "            return embedding, logits\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "# Triplet selection functions\n",
    "def create_triplets(embeddings, speaker_ids, mining='random'):\n",
    "    \"\"\"\n",
    "    Create triplets for triplet loss\n",
    "    mining: 'random', 'semi-hard', or 'hard'\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    emb_matrix = embeddings.detach()\n",
    "    \n",
    "    # Get unique speaker ids\n",
    "    unique_speakers = torch.unique(speaker_ids)\n",
    "    \n",
    "    # For each anchor\n",
    "    for speaker in unique_speakers:\n",
    "        # Get indices for this speaker\n",
    "        speaker_indices = (speaker_ids == speaker).nonzero().squeeze(1)\n",
    "        \n",
    "        # Need at least 2 examples of this speaker\n",
    "        if speaker_indices.numel() < 2:\n",
    "            continue\n",
    "            \n",
    "        # Get indices for other speakers\n",
    "        other_indices = (speaker_ids != speaker).nonzero().squeeze(1)\n",
    "        \n",
    "        # Need at least 1 example of another speaker\n",
    "        if other_indices.numel() < 1:\n",
    "            continue\n",
    "        \n",
    "        # For each potential anchor from this speaker\n",
    "        for i in range(speaker_indices.size(0)):\n",
    "            anchor_idx = speaker_indices[i].item()\n",
    "            anchor = emb_matrix[anchor_idx]\n",
    "            \n",
    "            # Choose positive (same speaker, different utterance)\n",
    "            pos_indices = [idx.item() for idx in speaker_indices if idx.item() != anchor_idx]\n",
    "            \n",
    "            if mining == 'hard' or mining == 'semi-hard':\n",
    "                # Calculate distances to all positives\n",
    "                pos_distances = []\n",
    "                for pos_idx in pos_indices:\n",
    "                    distance = torch.norm(anchor - emb_matrix[pos_idx])\n",
    "                    pos_distances.append((distance.item(), pos_idx))\n",
    "                \n",
    "                # Hard mining: choose furthest positive\n",
    "                if mining == 'hard':\n",
    "                    positive_idx = max(pos_distances, key=lambda x: x[0])[1]\n",
    "                else:  # semi-hard: choose random from top half\n",
    "                    pos_distances.sort(key=lambda x: x[0], reverse=True)\n",
    "                    positive_idx = pos_distances[random.randint(0, len(pos_distances)//2)][1]\n",
    "            else:\n",
    "                # Random mining: choose random positive\n",
    "                positive_idx = random.choice(pos_indices)\n",
    "            \n",
    "            positive = emb_matrix[positive_idx]\n",
    "            pos_distance = torch.norm(anchor - positive)\n",
    "            \n",
    "            if mining == 'hard' or mining == 'semi-hard':\n",
    "                # Calculate distances to all negatives\n",
    "                neg_distances = []\n",
    "                for neg_idx in other_indices:\n",
    "                    neg_idx = neg_idx.item()\n",
    "                    distance = torch.norm(anchor - emb_matrix[neg_idx])\n",
    "                    \n",
    "                    # For semi-hard: d(a,n) > d(a,p)\n",
    "                    if mining == 'semi-hard' and distance <= pos_distance:\n",
    "                        continue\n",
    "                        \n",
    "                    neg_distances.append((distance.item(), neg_idx))\n",
    "                \n",
    "                if not neg_distances:\n",
    "                    continue  # No suitable negative found\n",
    "                \n",
    "                if mining == 'hard':\n",
    "                    # Hard mining: choose closest negative\n",
    "                    negative_idx = min(neg_distances, key=lambda x: x[0])[1]\n",
    "                else:  # semi-hard: choose random from bottom half\n",
    "                    neg_distances.sort(key=lambda x: x[0])\n",
    "                    n_select = max(1, len(neg_distances)//2)\n",
    "                    negative_idx = neg_distances[random.randint(0, n_select-1)][1]\n",
    "            else:\n",
    "                # Random mining: choose random negative\n",
    "                negative_idx = other_indices[torch.randint(0, other_indices.size(0), (1,))].item()\n",
    "            \n",
    "            triplets.append((anchor_idx, positive_idx, negative_idx))\n",
    "    \n",
    "    if not triplets:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Extract the actual embeddings for each index\n",
    "    anchors = torch.stack([embeddings[i] for i, _, _ in triplets])\n",
    "    positives = torch.stack([embeddings[j] for _, j, _ in triplets])\n",
    "    negatives = torch.stack([embeddings[k] for _, _, k in triplets])\n",
    "    \n",
    "    return anchors, positives, negatives\n",
    "\n",
    "# Combined loss function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, margin=0.2, lambda_cls=1.0, lambda_triplet=1.0):\n",
    "        super().__init__()\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "        self.triplet_loss = nn.TripletMarginLoss(margin=margin)\n",
    "        self.lambda_cls = lambda_cls\n",
    "        self.lambda_triplet = lambda_triplet\n",
    "    \n",
    "    def forward(self, embeddings, logits, targets, mining='semi-hard'):\n",
    "        cls_loss = self.classification_loss(logits, targets)\n",
    "        \n",
    "        # Create triplets\n",
    "        anchors, positives, negatives = create_triplets(embeddings, targets, mining=mining)\n",
    "        \n",
    "        # If no triplets could be formed\n",
    "        if anchors is None:\n",
    "            return cls_loss\n",
    "        \n",
    "        trip_loss = self.triplet_loss(anchors, positives, negatives)\n",
    "        \n",
    "        return self.lambda_cls * cls_loss + self.lambda_triplet * trip_loss\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, \n",
    "                criterion, device, epochs=30, mining='semi-hard', \n",
    "                augmentation=True, grad_clip=3.0):\n",
    "    \"\"\"\n",
    "    Training loop for speaker embedding model\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        st = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for waveforms, speaker_ids in train_loader:\n",
    "            waveforms = waveforms.to(device)\n",
    "            speaker_ids = speaker_ids.to(device)\n",
    "            \n",
    "            # Apply augmentation if enabled\n",
    "            if augmentation:\n",
    "                waveforms = torch.stack([apply_augmentation(w) for w in waveforms])\n",
    "            \n",
    "            # Forward pass\n",
    "            embeddings, logits = model(waveforms)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(embeddings, logits, speaker_ids, mining=mining)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / batch_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for waveforms, speaker_ids in val_loader:\n",
    "                waveforms = waveforms.to(device)\n",
    "                speaker_ids = speaker_ids.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                embeddings, logits = model(waveforms)\n",
    "                \n",
    "                # Calculate loss (without triplet during validation for simplicity)\n",
    "                loss = nn.CrossEntropyLoss()(logits, speaker_ids)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                batch_count += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / batch_count\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.4f}')\n",
    "        print(f'  Time Taken: {time.time() - st}')\n",
    "        # Compute metrics every 5 epochs or last epoch\n",
    "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "            accuracy = compute_accuracy(model, val_loader, device)\n",
    "            print(f'  Validation Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Plot training curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Compute accuracy on a dataset\n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for waveforms, speaker_ids in dataloader:\n",
    "            waveforms = waveforms.to(device)\n",
    "            speaker_ids = speaker_ids.to(device)\n",
    "            \n",
    "            _, logits = model(waveforms)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            \n",
    "            total += speaker_ids.size(0)\n",
    "            correct += (predicted == speaker_ids).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "# Compute Equal Error Rate (EER)\n",
    "def compute_eer(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_speakers = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for waveforms, speaker_ids in dataloader:\n",
    "            waveforms = waveforms.to(device)\n",
    "            \n",
    "            if model.num_speakers:\n",
    "                embeddings, _ = model(waveforms)\n",
    "            else:\n",
    "                embeddings = model(waveforms)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_speakers.append(speaker_ids)\n",
    "    \n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    all_speakers = torch.cat(all_speakers, dim=0)\n",
    "    \n",
    "    # Compute all pairwise scores\n",
    "    scores = []\n",
    "    labels = []\n",
    "    \n",
    "    num_embeddings = len(all_embeddings)\n",
    "    for i in range(num_embeddings):\n",
    "        for j in range(i+1, num_embeddings):\n",
    "            sim = torch.cosine_similarity(\n",
    "                all_embeddings[i].unsqueeze(0), \n",
    "                all_embeddings[j].unsqueeze(0)\n",
    "            ).item()\n",
    "            scores.append(sim)\n",
    "            # 1 if same speaker, 0 if different\n",
    "            labels.append(1 if all_speakers[i] == all_speakers[j] else 0)\n",
    "    \n",
    "    # Compute EER\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    fnr = 1 - tpr\n",
    "    \n",
    "    # Find threshold where FPR = FNR (EER)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    \n",
    "    return eer\n",
    "\n",
    "# Extract and visualize embeddings\n",
    "def visualize_embeddings(model, dataloader, device, output_file='embeddings_visualization.png'):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_speakers = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for waveforms, speaker_ids in dataloader:\n",
    "            waveforms = waveforms.to(device)\n",
    "            \n",
    "            if model.num_speakers:\n",
    "                embeddings, _ = model(waveforms)\n",
    "            else:\n",
    "                embeddings = model(waveforms)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            all_speakers.append(speaker_ids.numpy())\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    all_speakers = np.concatenate(all_speakers)\n",
    "    \n",
    "    # Reduce dimensionality with t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_embeddings)-1))\n",
    "    embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    unique_speakers = np.unique(all_speakers)\n",
    "    \n",
    "    for speaker in unique_speakers:\n",
    "        mask = all_speakers == speaker\n",
    "        plt.scatter(\n",
    "            embeddings_2d[mask, 0], \n",
    "            embeddings_2d[mask, 1], \n",
    "            label=f'Speaker {speaker}',\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title('Speaker Embeddings Visualization (t-SNE)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "    \n",
    "    return embeddings_2d, all_speakers\n",
    "\n",
    "def load_model(model_path, device=None):\n",
    "    \"\"\"\n",
    "    Load a trained speaker embedding model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model file\n",
    "        device: Device to load the model on ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        Loaded model\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the saved model checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Create model with the same parameters\n",
    "    model = SpeakerEmbeddingModel(\n",
    "        embedding_dim=checkpoint['embedding_dim'],\n",
    "        num_speakers=checkpoint['num_speakers']\n",
    "    )\n",
    "    \n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Function to extract embedding for a single waveform\n",
    "def extract_embedding(model, waveform, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Ensure waveform has batch dimension\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        \n",
    "        waveform = waveform.to(device)\n",
    "        \n",
    "        if model.num_speakers:\n",
    "            embedding, _ = model(waveform)\n",
    "        else:\n",
    "            embedding = model(waveform)\n",
    "            \n",
    "    return embedding.cpu().numpy()\n",
    "\n",
    "# Main function to train model and extract embeddings\n",
    "def main(df, embedding_dim=256, batch_size=32, epochs=30, learning_rate=0.001, \n",
    "         mining='semi-hard', augmentation=True):\n",
    "    \"\"\"\n",
    "    Main function to train speaker embedding model and extract embeddings\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame with columns 'speech', 'speaker_id', 'instance_id'\n",
    "        embedding_dim: Dimension of speaker embeddings\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Number of training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        mining: Triplet mining strategy ('random', 'semi-hard', 'hard')\n",
    "        augmentation: Whether to use data augmentation\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create numeric speaker IDs if needed\n",
    "    if not pd.api.types.is_numeric_dtype(df['speaker_id']):\n",
    "        speaker_to_id = {speaker: idx for idx, speaker in enumerate(df['speaker_id'].unique())}\n",
    "        df['speaker_id_numeric'] = df['speaker_id'].map(speaker_to_id)\n",
    "        print(f\"Mapped {len(speaker_to_id)} speakers to numeric IDs\")\n",
    "    else:\n",
    "        df['speaker_id_numeric'] = df['speaker_id']\n",
    "    \n",
    "    # Split data into train, validation, and test\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, \n",
    "                                         stratify=df['speaker_id_numeric'])\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42, \n",
    "                                        stratify=train_df['speaker_id_numeric'])\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SpeakerDataset(train_df)\n",
    "    val_dataset = SpeakerDataset(val_df)\n",
    "    test_dataset = SpeakerDataset(test_df)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                           num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_speakers = len(df['speaker_id_numeric'].unique())\n",
    "    model = SpeakerEmbeddingModel(embedding_dim=embedding_dim, num_speakers=num_speakers, \n",
    "                                 unfreeze_layers=2)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"Model initialized with {embedding_dim} dimensional embeddings\")\n",
    "    print(f\"Number of speakers: {num_speakers}\")\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} \"\n",
    "          f\"({100 * trainable_params / total_params:.2f}%)\")\n",
    "    \n",
    "    # Create optimizer with different learning rates for different parts\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.attention.parameters(), 'lr': learning_rate},\n",
    "        {'params': model.projector.parameters(), 'lr': learning_rate},\n",
    "        {'params': model.classifier.parameters(), 'lr': learning_rate},\n",
    "        {'params': model.wav2vec.parameters(), 'lr': learning_rate * 0.1}\n",
    "    ])\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', patience=3, factor=0.5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = CombinedLoss(margin=0.2, lambda_cls=1.0, lambda_triplet=0.5)\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    model, train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader, optimizer, scheduler, criterion, \n",
    "        device, epochs=epochs, mining=mining, augmentation=augmentation\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_accuracy = compute_accuracy(model, test_loader, device)\n",
    "    print(f\"Test accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Try to compute EER (may fail if not enough speakers/samples)\n",
    "    try:\n",
    "        eer = compute_eer(model, test_loader, device)\n",
    "        print(f\"Equal Error Rate: {eer:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute EER: {e}\")\n",
    "    \n",
    "    # Visualize embeddings\n",
    "    print(\"Generating embedding visualization...\")\n",
    "    visualize_embeddings(model, test_loader, device)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = 'speaker_embedding_model.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'num_speakers': num_speakers,\n",
    "    }, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Helper function to extract embeddings from the trained model\n",
    "def extract_all_embeddings(model, df, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extract embeddings for all waveforms in the dataframe\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with original columns plus 'embedding'\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    dataset = SpeakerDataset(df)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for waveforms, _ in dataloader:\n",
    "            waveforms = waveforms.to(device)\n",
    "            \n",
    "            if model.num_speakers:\n",
    "                embeddings, _ = model(waveforms)\n",
    "            else:\n",
    "                embeddings = model(waveforms)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    # Add embeddings to dataframe\n",
    "    result_df = df.copy()\n",
    "    result_df['embedding'] = list(all_embeddings)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ef3867-7440-4d02-8ce2-f4312924397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from SE_W2V2/2/speaker_embedding_model.pth\n",
      "Generated embeddings for 30892 samples\n",
      "Embeddings saved to speaker_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df is already loaded with columns:\n",
    "    # - speech: preprocessed waveforms (torch tensors or numpy arrays)\n",
    "    # - speaker_id: ID of the speaker\n",
    "    # - instance_id: ID of the specific utterance\n",
    "    # If you need to test with synthetic data, uncomment this:\n",
    "    \"\"\"\n",
    "    # Create synthetic data for testing\n",
    "    num_speakers = 20\n",
    "    samples_per_speaker = 10\n",
    "    waveform_length = 16000  # 1 second at 16kHz\n",
    "    \n",
    "    synthetic_data = []\n",
    "    for speaker_id in range(num_speakers):\n",
    "        for instance_id in range(samples_per_speaker):\n",
    "            # Create synthetic waveform with speaker-specific frequency\n",
    "            freq = 100 + speaker_id * 20  # Different frequency for each speaker\n",
    "            t = np.linspace(0, 1, waveform_length)\n",
    "            waveform = np.sin(2 * np.pi * freq * t)\n",
    "            # Add some noise\n",
    "            waveform += np.random.normal(0, 0.1, waveform_length)\n",
    "            # Convert to torch tensor\n",
    "            waveform = torch.FloatTensor(waveform)\n",
    "            \n",
    "            synthetic_data.append({\n",
    "                'speech': waveform,\n",
    "                'speaker_id': speaker_id,\n",
    "                'instance_id': instance_id\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(synthetic_data)\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Train the model\n",
    "    # model = main(\n",
    "    #     df, \n",
    "    #     embedding_dim=256, \n",
    "    #     batch_size=32, \n",
    "    #     epochs=20, \n",
    "    #     learning_rate=0.001,\n",
    "    #     mining='semi-hard',\n",
    "    #     augmentation=False\n",
    "    # )\n",
    "\n",
    "    # Load the model\n",
    "    model_path = 'SE_W2V2/2/speaker_embedding_model.pth'\n",
    "    model = load_model(model_path)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    # Extract embeddings for all data\n",
    "    embeddings_df = extract_all_embeddings(model, df)\n",
    "    print(f\"Generated embeddings for {len(embeddings_df)} samples\")\n",
    "    \n",
    "    # Save embeddings\n",
    "    embeddings_df.to_pickle('speaker_embeddings.pkl')\n",
    "    print(\"Embeddings saved to speaker_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe4a7d6-d0b2-460f-a4f2-f43930047091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>text</th>\n",
       "      <th>speech</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>001</td>\n",
       "      <td>please call stella</td>\n",
       "      <td>[tensor(-0.0024), tensor(-0.0035), tensor(-0.0...</td>\n",
       "      <td>[0.027370663, -0.0525528, -0.032500908, 0.0376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>002</td>\n",
       "      <td>ask her to bring these things with her from th...</td>\n",
       "      <td>[tensor(-0.0002), tensor(3.0518e-05), tensor(-...</td>\n",
       "      <td>[-0.06583665, 0.03920007, -0.09141751, 0.02472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>003</td>\n",
       "      <td>six spoons of fresh snow peas five thick slabs...</td>\n",
       "      <td>[tensor(0.0026), tensor(0.0028), tensor(0.0027...</td>\n",
       "      <td>[0.008412681, -0.008078956, -0.1015041, 0.0674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>004</td>\n",
       "      <td>we also need a small plastic snake and a big t...</td>\n",
       "      <td>[tensor(-0.0017), tensor(-0.0024), tensor(-0.0...</td>\n",
       "      <td>[-0.02975316, 0.017807143, -0.10778602, 0.0714...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>005</td>\n",
       "      <td>she can scoop these things into three red bags...</td>\n",
       "      <td>[tensor(-0.0088), tensor(-0.0092), tensor(-0.0...</td>\n",
       "      <td>[0.0065611075, 0.015251459, -0.11272573, 0.048...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speaker_id instance_id                                               text  \\\n",
       "0           0         001                                 please call stella   \n",
       "1           0         002  ask her to bring these things with her from th...   \n",
       "2           0         003  six spoons of fresh snow peas five thick slabs...   \n",
       "3           0         004  we also need a small plastic snake and a big t...   \n",
       "4           0         005  she can scoop these things into three red bags...   \n",
       "\n",
       "                                              speech  \\\n",
       "0  [tensor(-0.0024), tensor(-0.0035), tensor(-0.0...   \n",
       "1  [tensor(-0.0002), tensor(3.0518e-05), tensor(-...   \n",
       "2  [tensor(0.0026), tensor(0.0028), tensor(0.0027...   \n",
       "3  [tensor(-0.0017), tensor(-0.0024), tensor(-0.0...   \n",
       "4  [tensor(-0.0088), tensor(-0.0092), tensor(-0.0...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.027370663, -0.0525528, -0.032500908, 0.0376...  \n",
       "1  [-0.06583665, 0.03920007, -0.09141751, 0.02472...  \n",
       "2  [0.008412681, -0.008078956, -0.1015041, 0.0674...  \n",
       "3  [-0.02975316, 0.017807143, -0.10778602, 0.0714...  \n",
       "4  [0.0065611075, 0.015251459, -0.11272573, 0.048...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d82d1314-4ba2-41fd-9316-87d6e4f912ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate DataFrame size: 10.72 MB\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "size_bytes = sys.getsizeof(embeddings_df)\n",
    "print(f\"Approximate DataFrame size: {size_bytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fe8525d-091d-473c-8834-ff0d6ff47d55",
   "metadata": {},
   "source": [
    "01 epoch ~= 1169 ~ 19:29\n",
    "02 epoch ~= 1165 ~ 19:25\n",
    "03 epoch ~= 1165 ~ 19:25\n",
    "04 epoch ~= 1167 ~ 19:27\n",
    "05 epoch ~= 1167 ~ 19:27\n",
    "06 epoch ~= 1167 ~ 19:27\n",
    "07 epoch ~= 1167 ~ 19:27\n",
    "08 epoch ~= 1167 ~ 19:27\n",
    "09 epoch ~=  ~ \n",
    "10 epoch ~=  ~ \n",
    "11 epoch ~=  ~ \n",
    "12 epoch ~=  ~ \n",
    "13 epoch ~=  ~ \n",
    "14 epoch ~=  ~ \n",
    "15 epoch ~=  ~ \n",
    "16 epoch ~=  ~ \n",
    "17 epoch ~=  ~ \n",
    "18 epoch ~=  ~ \n",
    "19 epoch ~=  ~ \n",
    "20 epoch ~=  ~ \n",
    "\n",
    "\n",
    "# Train the model\n",
    "    model = main(\n",
    "        df, \n",
    "        embedding_dim=256, \n",
    "        batch_size=32, \n",
    "        epochs=20, \n",
    "        learning_rate=0.001,\n",
    "        mining='semi-hard',\n",
    "        augmentation=False\n",
    "    )\n",
    "\n",
    "Using device: cuda\n",
    "Train set: 18534 samples\n",
    "Validation set: 6179 samples\n",
    "Test set: 6179 samples\n",
    "Model initialized with 256 dimensional embeddings\n",
    "Number of speakers: 76\n",
    "Trainable parameters: 14,819,917 / 95,015,117 (15.60%)\n",
    "Starting training for 20 epochs...\n",
    "\n",
    "/home/sharma.harsh2/miniconda3/envs/myenv2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
    "  warnings.warn(\n",
    "\n",
    "Epoch 1/20:\n",
    "  Train Loss: 3.2056\n",
    "  Val Loss: 2.3962\n",
    "  Time Taken: 1173.0132133960724\n",
    "Epoch 2/20:\n",
    "  Train Loss: 1.7739\n",
    "  Val Loss: 1.4523\n",
    "  Time Taken: 1174.0224962234497\n",
    "Epoch 3/20:\n",
    "  Train Loss: 1.0008\n",
    "  Val Loss: 0.8897\n",
    "  Time Taken: 1173.9158658981323\n",
    "Epoch 4/20:\n",
    "  Train Loss: 0.6006\n",
    "  Val Loss: 0.5929\n",
    "  Time Taken: 1174.7569563388824\n",
    "Epoch 5/20:\n",
    "  Train Loss: 0.3986\n",
    "  Val Loss: 0.4580\n",
    "  Time Taken: 1174.8508441448212\n",
    "  Validation Accuracy: 95.23%\n",
    "Epoch 6/20:\n",
    "  Train Loss: 0.2912\n",
    "  Val Loss: 0.3700\n",
    "  Time Taken: 1174.3852279186249\n",
    "Epoch 7/20:\n",
    "  Train Loss: 0.2209\n",
    "  Val Loss: 0.3453\n",
    "  Time Taken: 1175.0321156978607\n",
    "Epoch 8/20:\n",
    "  Train Loss: 0.1769\n",
    "  Val Loss: 0.2491\n",
    "  Time Taken: 1174.7690753936768\n",
    "Epoch 9/20:\n",
    "  Train Loss: 0.1469\n",
    "  Val Loss: 0.2465\n",
    "  Time Taken: 1175.8201744556427\n",
    "Epoch 10/20:\n",
    "  Train Loss: 0.1290\n",
    "  Val Loss: 0.2334\n",
    "  Time Taken: 1175.1751747131348\n",
    "  Validation Accuracy: 96.07%\n",
    "Epoch 11/20:\n",
    "  Train Loss: 0.1126\n",
    "  Val Loss: 0.2082\n",
    "  Time Taken: 1173.8695333003998\n",
    "Epoch 12/20:\n",
    "  Train Loss: 0.0925\n",
    "  Val Loss: 0.2225\n",
    "  Time Taken: 1173.376716852188\n",
    "Epoch 13/20:\n",
    "  Train Loss: 0.0864\n",
    "  Val Loss: 0.1841\n",
    "  Time Taken: 1172.8260645866394\n",
    "Epoch 14/20:\n",
    "  Train Loss: 0.0777\n",
    "  Val Loss: 0.1502\n",
    "  Time Taken: 1174.054889202118\n",
    "Epoch 15/20:\n",
    "  Train Loss: 0.0707\n",
    "  Val Loss: 0.1583\n",
    "  Time Taken: 1174.5265798568726\n",
    "  Validation Accuracy: 96.63%\n",
    "Epoch 16/20:\n",
    "  Train Loss: 0.0668\n",
    "  Val Loss: 0.2548\n",
    "  Time Taken: 1175.542641878128\n",
    "Epoch 17/20:\n",
    "  Train Loss: 0.0638\n",
    "  Val Loss: 0.1822\n",
    "  Time Taken: 1176.1042213439941\n",
    "Epoch 18/20:\n",
    "  Train Loss: 0.0561\n",
    "  Val Loss: 0.1412\n",
    "  Time Taken: 1176.2044396400452\n",
    "Epoch 19/20:\n",
    "  Train Loss: 0.0532\n",
    "  Val Loss: 0.1485\n",
    "  Time Taken: 1176.0817244052887\n",
    "Epoch 20/20:\n",
    "  Train Loss: 0.0493\n",
    "  Val Loss: 0.1600\n",
    "  Time Taken: 1175.663535118103\n",
    "  Validation Accuracy: 96.59%\n",
    "Test accuracy: 96.13%\n",
    "Equal Error Rate: 0.0580\n",
    "Generating embedding visualization...\n",
    "Model saved to speaker_embedding_model.pth\n",
    "Generated embeddings for 30892 samplesr"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d58187d-578e-4662-845e-a27db37ea605",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1 epoch ~= 1143 ~ 19:03\n",
    "2 epoch ~= 1145 ~ 19:05\n",
    "3 epoch ~= 1146 ~ 19:06\n",
    "4 epoch ~= 1147 ~ 19:07\n",
    "5 epoch ~= 1149 ~ 19:09\n",
    "6 epoch ~= 1150 ~ 19:10\n",
    "7 epoch ~= 1194 ~ 19:54\n",
    "8 epoch ~= 1604 ~ 26:44\n",
    "9 epoch ~= 1348 ~ 22:28\n",
    "0 epoch ~= 1143 ~ 19:03\n",
    "\n",
    "# Train the model\n",
    "    model = main(\n",
    "        df, \n",
    "        embedding_dim=256, \n",
    "        batch_size=16, \n",
    "        epochs=10, \n",
    "        learning_rate=0.001,\n",
    "        mining='semi-hard',\n",
    "        augmentation=False\n",
    "    )\n",
    "\n",
    "Using device: cuda\n",
    "Train set: 18534 samples\n",
    "Validation set: 6179 samples\n",
    "Test set: 6179 samples\n",
    "Model initialized with 256 dimensional embeddings\n",
    "Number of speakers: 76\n",
    "Trainable parameters: 14,819,917 / 95,015,117 (15.60%)\n",
    "Starting training for 10 epochs...\n",
    "\n",
    "/home/sharma.harsh2/miniconda3/envs/myenv2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
    "  warnings.warn(\n",
    "\n",
    "Epoch 1/10:\n",
    "  Train Loss: 2.9894\n",
    "  Val Loss: 2.0630\n",
    "  Time Taken: 1143.8049654960632\n",
    "Epoch 2/10:\n",
    "  Train Loss: 1.4652\n",
    "  Val Loss: 1.0983\n",
    "  Time Taken: 1145.1459085941315\n",
    "Epoch 3/10:\n",
    "  Train Loss: 0.7894\n",
    "  Val Loss: 0.6629\n",
    "  Time Taken: 1146.2975623607635\n",
    "Epoch 4/10:\n",
    "  Train Loss: 0.4749\n",
    "  Val Loss: 0.4333\n",
    "  Time Taken: 1147.6955440044403\n",
    "Epoch 5/10:\n",
    "  Train Loss: 0.3300\n",
    "  Val Loss: 0.3754\n",
    "  Time Taken: 1149.554771900177\n",
    "  Validation Accuracy: 93.93%\n",
    "Epoch 6/10:\n",
    "  Train Loss: 0.2548\n",
    "  Val Loss: 0.3080\n",
    "  Time Taken: 1150.5601243972778\n",
    "Epoch 7/10:\n",
    "  Train Loss: 0.2079\n",
    "  Val Loss: 0.3189\n",
    "  Time Taken: 1194.53800034523\n",
    "Epoch 8/10:\n",
    "  Train Loss: 0.1753\n",
    "  Val Loss: 0.2328\n",
    "  Time Taken: 1604.399994134903\n",
    "Epoch 9/10:\n",
    "  Train Loss: 0.1465\n",
    "  Val Loss: 0.2221\n",
    "  Time Taken: 1348.591305732727\n",
    "Epoch 10/10:\n",
    "  Train Loss: 0.1349\n",
    "  Val Loss: 0.2178\n",
    "  Time Taken: 1143.7896163463593\n",
    "  Validation Accuracy: 95.05%\n",
    "Test accuracy: 94.95%\n",
    "Equal Error Rate: 0.0600\n",
    "Generating embedding visualization...\n",
    "Model saved to speaker_embedding_model.pth\n",
    "Generated embeddings for 30892 samples"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1b26479-a26e-4f68-9f88-e4a0f91383c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1 epoch ~= 1070 ~ 18 \n",
    "2 epoch ~= 1950 ~ (1950 -1070) = 880 ~ 15\n",
    "3 epoch ~= 19.36  2780\n",
    "4 epoch ~= 19.25 3838\n",
    "\n",
    "# Train the model\n",
    "    model = main(\n",
    "        df, \n",
    "        embedding_dim=256, \n",
    "        batch_size=32, \n",
    "        epochs=30, \n",
    "        learning_rate=0.001,\n",
    "        mining='semi-hard',\n",
    "        augmentation=False\n",
    "    )\n",
    "\n",
    "Using device: cuda\n",
    "Train set: 18534 samples\n",
    "Validation set: 6179 samples\n",
    "Test set: 6179 samples\n",
    "Model initialized with 256 dimensional embeddings\n",
    "Number of speakers: 76\n",
    "Trainable parameters: 14,819,917 / 95,015,117 (15.60%)\n",
    "Starting training for 30 epochs...\n",
    "\n",
    "/home/sharma.harsh2/miniconda3/envs/myenv2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
    "  warnings.warn(\n",
    "\n",
    "Epoch 1/30:\n",
    "  Train Loss: 3.2056\n",
    "  Val Loss: 2.3962\n",
    "Epoch 2/30:\n",
    "  Train Loss: 1.7739\n",
    "  Val Loss: 1.4523\n",
    "Epoch 3/30:\n",
    "  Train Loss: 1.0008\n",
    "  Val Loss: 0.8897\n",
    "Epoch 4/30:\n",
    "  Train Loss: 0.6006\n",
    "  Val Loss: 0.5929"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9eacf6b8-87b9-4c3a-97ac-ecd57fcb6316",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1 epoch ~= 20\n",
    "2 epoch ~= 18\n",
    "3 epoch ~= 17\n",
    "4 epoch ~= 17\n",
    "\n",
    "# Train the model\n",
    "    model = main(\n",
    "        df, \n",
    "        embedding_dim=256, \n",
    "        batch_size=32, \n",
    "        epochs=30, \n",
    "        learning_rate=0.001,\n",
    "        mining='semi-hard',\n",
    "        augmentation=True\n",
    "    )\n",
    "\n",
    "Using device: cuda\n",
    "Train set: 18534 samples\n",
    "Validation set: 6179 samples\n",
    "Test set: 6179 samples\n",
    "Model initialized with 256 dimensional embeddings\n",
    "Number of speakers: 76\n",
    "Trainable parameters: 14,819,917 / 95,015,117 (15.60%)\n",
    "Starting training for 30 epochs...\n",
    "\n",
    "/home/sharma.harsh2/miniconda3/envs/myenv2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
    "  warnings.warn(\n",
    "\n",
    "Epoch 1/30:\n",
    "  Train Loss: 3.3997\n",
    "  Val Loss: 2.6313\n",
    "Epoch 2/30:\n",
    "  Train Loss: 2.1623\n",
    "  Val Loss: 1.7037\n",
    "Epoch 3/30:\n",
    "  Train Loss: 1.4566\n",
    "  Val Loss: 1.1670\n",
    "Epoch 4/30:\n",
    "  Train Loss: 1.0371\n",
    "  Val Loss: 0.8462"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
